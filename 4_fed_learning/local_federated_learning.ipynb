{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the MNIST dataset (or any other dataset like HAM 10000)\n",
    "2. Extract two subsets of 600 data points each (without intersection)\n",
    "3. Create a simple Convolutional Neural Network (2 convolutional layers and 2 dense layers, for example)\n",
    "4. Create a function average_model_parameters(models: iterable, average_weight): iterable that takes a list of models as an argument and returns the weighted average of the parameters of each model.\n",
    "5. Create a function that updates the parameters of a model from a list of values\n",
    "6. Create a script/code/function that reproduces Algorithm 1, considering that both models are on your machine. Use an average_weight=[1/2, 1/2]. Reuse the same setup as in the article (50 examples per local batch)\n",
    "7. Train your models without initializing the common parameters and measure the performance on the entire dataset.\n",
    "8. Train your models with the initialization of common parameters and verify that the performance is better.\n",
    "9. Reduce the number of data points in each sub-batch. What is the minimum number of data points necessary for the final model to have acceptable performance? Repeat the study on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract two subsets of 600 data points each (without intersection)\n",
    "subset1, subset2 = random_split(train_dataset, [600, len(train_dataset) - 600])\n",
    "subset2, rest = random_split(subset2, [600, len(subset2) - 600])\n",
    "\n",
    "train_loader1 = DataLoader(subset1, batch_size=50, shuffle=True)\n",
    "train_loader2 = DataLoader(subset2, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a simple Convolutional Neural Network (2 convolutional layers and 2 dense layers, for example)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size=1):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "def train_cnn(model, train_loader, epochs=10, lr=1e-3):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "        avg_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch}, avg_loss: {avg_loss}\")\n",
    "\n",
    "def evaluate_cnn(model, test_loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a function average_model_parameters(models: iterable, average_weight): iterable that takes a list of models as an argument and returns the weighted average of the parameters of each model.\n",
    "def average_model_parameters(models: list, average_weight: list[float]):\n",
    "    n_models = len(models)\n",
    "    for param in zip(*[model.parameters() for model in models]):\n",
    "        tmp = torch.zeros_like(param[0])\n",
    "        for i in range(n_models):\n",
    "            tmp += param[i] * average_weight[i]\n",
    "        yield tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create a function that updates the parameters of a model from a list of values\n",
    "def update_model_parameters(model, values):\n",
    "    for model_params, value in zip(model.parameters(), values):\n",
    "        model_params.data = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create a script/code/function that reproduces Algorithm 1, considering that both models are on your machine. Use an average_weight=[1/2, 1/2]. Reuse the same setup as in the article (50 examples per local batch)\n",
    "import random\n",
    "def server_update(models: list, train_loaders, C = 0.8, K = 10, n_rounds = 2, input_size=1):\n",
    "    central_model = CNN(input_size)\n",
    "    for _ in range(n_rounds):\n",
    "        m = max(int(C * K), 1)\n",
    "        S = random.sample(range(K), m)\n",
    "        models_selected = [models[k] for k in S]\n",
    "        for client_k in S:\n",
    "            client_update(models, train_loaders, client_k, central_model.state_dict())\n",
    "        update_model_parameters(central_model, list(average_model_parameters(models_selected, [1/m] * m)))\n",
    "    return central_model\n",
    "        \n",
    "def client_update(models: list, train_loaders: list, k: int, w):\n",
    "    model = models[k]\n",
    "    model.load_state_dict(w)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(10):\n",
    "        for images, labels in train_loaders[k]:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, avg_loss: 2.0143082439899445\n",
      "Epoch 1, avg_loss: 0.999852642416954\n",
      "Epoch 2, avg_loss: 0.5145983522137006\n",
      "Epoch 3, avg_loss: 0.36232953642805416\n",
      "Epoch 4, avg_loss: 0.2512669339776039\n",
      "Epoch 5, avg_loss: 0.1779667946199576\n",
      "Epoch 6, avg_loss: 0.14279506107171377\n",
      "Epoch 7, avg_loss: 0.09343061254670222\n",
      "Epoch 8, avg_loss: 0.06588266743347049\n",
      "Epoch 9, avg_loss: 0.044175000550846256\n",
      "Epoch 10, avg_loss: 0.032474757016946874\n",
      "Epoch 11, avg_loss: 0.023645147603626054\n",
      "Epoch 12, avg_loss: 0.014716748303423325\n",
      "Epoch 13, avg_loss: 0.011454202137732258\n",
      "Epoch 14, avg_loss: 0.008813771981901178\n",
      "Epoch 15, avg_loss: 0.007679475781818231\n",
      "Epoch 16, avg_loss: 0.005148227928051104\n",
      "Epoch 17, avg_loss: 0.004345070008033265\n",
      "Epoch 18, avg_loss: 0.003678150572037945\n",
      "Epoch 19, avg_loss: 0.0032373669770701476\n",
      "Epoch 0, avg_loss: 2.0800111095110574\n",
      "Epoch 1, avg_loss: 1.0061410864194233\n",
      "Epoch 2, avg_loss: 0.5042655194799105\n",
      "Epoch 3, avg_loss: 0.3519003912806511\n",
      "Epoch 4, avg_loss: 0.22582276413838068\n",
      "Epoch 5, avg_loss: 0.20279498274127641\n",
      "Epoch 6, avg_loss: 0.1762768023957809\n",
      "Epoch 7, avg_loss: 0.12573206331580877\n",
      "Epoch 8, avg_loss: 0.0886128939067324\n",
      "Epoch 9, avg_loss: 0.05146710015833378\n",
      "Epoch 10, avg_loss: 0.03676955813231567\n",
      "Epoch 11, avg_loss: 0.02581728028599173\n",
      "Epoch 12, avg_loss: 0.017358559959878523\n",
      "Epoch 13, avg_loss: 0.013956680428236723\n",
      "Epoch 14, avg_loss: 0.010760194466759762\n",
      "Epoch 15, avg_loss: 0.008251844168019792\n",
      "Epoch 16, avg_loss: 0.006743084969154249\n",
      "Epoch 17, avg_loss: 0.005567096445399026\n",
      "Epoch 18, avg_loss: 0.00479773135157302\n",
      "Epoch 19, avg_loss: 0.004406668750258784\n",
      "Accuracy of model 1: 0.9412\n",
      "Accuracy of model 2: 0.943\n",
      "Accuracy of the average model: 0.2796\n"
     ]
    }
   ],
   "source": [
    "# 7. Train your models without initializing the common parameters and measure the performance on the entire dataset.\n",
    "cnn1 = CNN()\n",
    "cnn2 = CNN()\n",
    "train_cnn(cnn1, train_loader1, epochs=20)\n",
    "train_cnn(cnn2, train_loader2, epochs=20)\n",
    "\n",
    "acc_1 = evaluate_cnn(cnn1, DataLoader(test_dataset, batch_size=50))\n",
    "acc_2 = evaluate_cnn(cnn2, DataLoader(test_dataset, batch_size=50))\n",
    "print(f\"Accuracy of model 1: {acc_1}\")\n",
    "print(f\"Accuracy of model 2: {acc_2}\")\n",
    "\n",
    "average_model = CNN()\n",
    "update_model_parameters(average_model, list(average_model_parameters([cnn1, cnn2], [0.5, 0.5])))\n",
    "acc_avg = evaluate_cnn(average_model, DataLoader(test_dataset, batch_size=50))\n",
    "print(f\"Accuracy of the average model: {acc_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple averaging is bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Train your models with the initialization of common parameters and verify that the performance is better.\n",
    "K = 2\n",
    "models = [CNN() for _ in range(K)]\n",
    "train_loaders = [train_loader1, train_loader2]\n",
    "server_model = server_update(models, train_loaders, C=0.8, K=K, n_rounds=2)\n",
    "acc_server = evaluate_cnn(server_model, DataLoader(test_dataset, batch_size=50))\n",
    "client_models_acc = [evaluate_cnn(model, DataLoader(test_dataset, batch_size=50)) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model 1: 0.9539\n",
      "Accuracy of model 2: 0.9351\n",
      "Accuracy of the server model: 0.9539\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of model 1: {client_models_acc[0]}\")\n",
    "print(f\"Accuracy of model 2: {client_models_acc[1]}\")\n",
    "print(f\"Accuracy of the server model: {acc_server}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the server model with batch size 1: 0.9192\n",
      "Accuracy of the server model with batch size 5: 0.9498\n",
      "Accuracy of the server model with batch size 10: 0.9512\n",
      "Accuracy of the server model with batch size 20: 0.9559\n",
      "Accuracy of the server model with batch size 50: 0.9555\n"
     ]
    }
   ],
   "source": [
    "# 9. Reduce the number of data points in each sub-batch. What is the minimum number of data points necessary for the final model to have acceptable performance?\n",
    "def test_different_batch_sizes(K=2, C=0.8, n_rounds=2):\n",
    "    batch_sizes = [1, 5, 10, 20, 50]\n",
    "    models = [CNN() for _ in range(K)]\n",
    "    for batch_size in batch_sizes:\n",
    "        train_loaders = [DataLoader(subset, batch_size=batch_size, shuffle=True) for subset in [subset1, subset2]]\n",
    "        server_model = server_update(models, train_loaders, C=C, K=K, n_rounds=n_rounds)\n",
    "        acc_server = evaluate_cnn(server_model, DataLoader(test_dataset, batch_size=50))\n",
    "        print(f\"Accuracy of the server model with batch size {batch_size}: {acc_server}\")\n",
    "\n",
    "test_different_batch_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the sub-batch size doesn't seem to affect the performance whatsoever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Repeat the study with cifar10\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "subset1, subset2 = random_split(train_dataset, [600, len(train_dataset) - 600])\n",
    "subset2, rest = random_split(subset2, [600, len(subset2) - 600])\n",
    "\n",
    "train_loader1 = DataLoader(subset1, batch_size=50, shuffle=True)\n",
    "train_loader2 = DataLoader(subset2, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cifar10CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(1600, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 1600)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_update(models: list, train_loaders, C = 0.8, K = 10, n_rounds = 2, input_size=1):\n",
    "    central_model = Cifar10CNN()\n",
    "    for _ in range(n_rounds):\n",
    "        m = max(int(C * K), 1)\n",
    "        S = random.sample(range(K), m)\n",
    "        models_selected = [models[k] for k in S]\n",
    "        for client_k in S:\n",
    "            client_update(models, train_loaders, client_k, central_model.state_dict())\n",
    "        update_model_parameters(central_model, list(average_model_parameters(models_selected, [1/m] * m)))\n",
    "    return central_model\n",
    "        \n",
    "def client_update(models: list, train_loaders: list, k: int, w):\n",
    "    model = models[k]\n",
    "    model.load_state_dict(w)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    for epoch in range(10):\n",
    "        for images, labels in train_loaders[k]:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the server model with batch size 1: 0.3002\n",
      "Accuracy of the server model with batch size 5: 0.3386\n",
      "Accuracy of the server model with batch size 10: 0.4101\n",
      "Accuracy of the server model with batch size 20: 0.3678\n",
      "Accuracy of the server model with batch size 50: 0.3644\n"
     ]
    }
   ],
   "source": [
    "def test_different_batch_sizes_cifar(K=2, C=0.8, n_rounds=2):\n",
    "    batch_sizes = [1, 5, 10, 20, 50]\n",
    "    models = [Cifar10CNN() for _ in range(K)]\n",
    "    for batch_size in batch_sizes:\n",
    "        train_loaders = [DataLoader(subset, batch_size=batch_size, shuffle=True) for subset in [subset1, subset2]]\n",
    "        server_model = server_update(models, train_loaders, C=C, K=K, n_rounds=n_rounds)\n",
    "        acc_server = evaluate_cnn(server_model, DataLoader(test_dataset, batch_size=50))\n",
    "        print(f\"Accuracy of the server model with batch size {batch_size}: {acc_server}\")\n",
    "\n",
    "test_different_batch_sizes_cifar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, avg_loss: 2.253656884034475\n",
      "Epoch 1, avg_loss: 2.0900201201438904\n",
      "Epoch 2, avg_loss: 1.9536284903685253\n",
      "Epoch 3, avg_loss: 1.8264440695444744\n",
      "Epoch 4, avg_loss: 1.6993524432182312\n",
      "Epoch 5, avg_loss: 1.5973156690597534\n",
      "Epoch 6, avg_loss: 1.4910090565681458\n",
      "Epoch 7, avg_loss: 1.3618240753809612\n",
      "Epoch 8, avg_loss: 1.2309393286705017\n",
      "Epoch 9, avg_loss: 1.1016557763020198\n",
      "Epoch 10, avg_loss: 1.0436801314353943\n",
      "Epoch 11, avg_loss: 0.9547400325536728\n",
      "Epoch 12, avg_loss: 0.8242685794830322\n",
      "Epoch 13, avg_loss: 0.6593535989522934\n",
      "Epoch 14, avg_loss: 0.5840179920196533\n",
      "Epoch 15, avg_loss: 0.4907771398623784\n",
      "Epoch 16, avg_loss: 0.4069172367453575\n",
      "Epoch 17, avg_loss: 0.3123160886267821\n",
      "Epoch 18, avg_loss: 0.20530709127585092\n",
      "Epoch 19, avg_loss: 0.1638720209399859\n",
      "Accuracy of the simple model: 0.3461\n"
     ]
    }
   ],
   "source": [
    "simple_model = Cifar10CNN()\n",
    "train_cnn(simple_model, train_loader1, epochs=20)\n",
    "acc_simple = evaluate_cnn(simple_model, DataLoader(test_dataset, batch_size=50))\n",
    "print(f\"Accuracy of the simple model: {acc_simple}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance are worse than for mnist but training the model as is without client-server, perform the same?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
